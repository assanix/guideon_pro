import asyncio
import logging
import os
import re
import hashlib
from typing import Dict, Any, List

import openai
import google.generativeai as genai
import motor.motor_asyncio
import tiktoken
import docx
import PyPDF2
import openpyxl
from aiogram import Bot, Dispatcher, types, Router, F
from aiogram.filters import Command
from aiogram.types import KeyboardButton, ReplyKeyboardMarkup, InputMediaPhoto, FSInputFile
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.context import FSMContext
from dotenv import load_dotenv
from pinecone import Pinecone
import aiofiles

load_dotenv()

# Configuration
MONGO_URI = os.getenv("MONGO_URI")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
API_TOKEN = os.getenv("API_TOKEN")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENV = os.getenv("PINECONE_ENV")
PINECONE_INDEX_NAME = "kbtu-docs"
GEMINI_API_KEY=os.getenv("GEMINI_API_KEY")


# Initialize Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(PINECONE_INDEX_NAME)

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Initialize OpenAI
openai.api_key = OPENAI_API_KEY

# Initialize Gemini
genai.configure(api_key=GEMINI_API_KEY)


# Initialize MongoDB
client = motor.motor_asyncio.AsyncIOMotorClient(MONGO_URI)
db = client["telegram_bot_db"]
users_collection = db["users"]
feedback_collection = db["feedback"]

# Initialize bot and dispatcher
bot = Bot(token=API_TOKEN)
dp = Dispatcher()
router = Router()

# Admins ids
ADMINS = [1138549375]

# Define the keyboard layouts
main_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [KeyboardButton(text="ğŸ—ºï¸ ĞšĞ°Ñ€Ñ‚Ğ°"), KeyboardButton(text="ğŸ“š Ğ Ğ£ĞŸ Ğ¨Ğ˜Ğ¢Ğ¸Ğ˜")],
        [KeyboardButton(text="ğŸ«£ Ğ“Ğ´Ğµ Ğ¯?"), KeyboardButton(text="ğŸ” ĞĞ°Ğ¹Ñ‚Ğ¸"), KeyboardButton(text="ğŸ¤– MentorGPT")],
        [KeyboardButton(text="ğŸ“¥ Ğ–Ğ°Ğ»Ğ¾Ğ±Ñ‹/ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ"), KeyboardButton(text="ğŸ’¬ ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹")]
    ],
    resize_keyboard=True
)

chatgpt_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [KeyboardButton(text="â¬…ï¸ ĞĞ°Ğ·Ğ°Ğ´")]
    ],
    resize_keyboard=True
)

find_keyboard = ReplyKeyboardMarkup(
    keyboard=[
        [KeyboardButton(text="ĞšÑƒÑˆĞ°Ñ‚ÑŒ"), KeyboardButton(text="Ğ£Ñ‡Ğ¸Ñ‚ÑŒÑÑ")],
        [KeyboardButton(text="ĞĞ°Ğ·Ğ°Ğ´")]
    ],
    resize_keyboard=True
)

class UnicodeFormatter(logging.Formatter):
    def format(self, record):
        if isinstance(record.msg, str):
            record.msg = record.msg.encode('utf-8').decode('unicode_escape')
        return super(UnicodeFormatter, self).format(record)

for handler in logger.handlers:
    handler.setFormatter(UnicodeFormatter())

async def clear_pinecone_index():
    try:
        index.delete(delete_all=True)
        logger.debug("All vectors deleted from Pinecone index.")
    except Exception as e:
        logger.error(f"Error deleting vectors from Pinecone: {e}")
        raise

async def reprocess_uploads():
    upload_dir = "./uploads/"
    files = os.listdir(upload_dir)
    for file_name in files:
        file_path = os.path.join(upload_dir, file_name)
        await FileHandler.process_and_store_document(file_path, file_name, 0)

async def handle_main_menu_button(message: types.Message, state: FSMContext):
    await state.clear()
    if message.text == "ğŸ—ºï¸ ĞšĞ°Ñ€Ñ‚Ğ°":
        await MessageHandler.handle_map(message)
    elif message.text == "ğŸ“š Ğ Ğ£ĞŸ Ğ¨Ğ˜Ğ¢Ğ¸Ğ˜":
        await MessageHandler.handle_rup(message)
    elif message.text == "ğŸ«£ Ğ“Ğ´Ğµ Ğ¯?":
        await MessageHandler.ask_for_room_number(message, state)
    elif message.text == "ğŸ” ĞĞ°Ğ¹Ñ‚Ğ¸":
        await MessageHandler.handle_find_room(message, state)
    elif message.text == "ğŸ“¥ Ğ–Ğ°Ğ»Ğ¾Ğ±Ñ‹/ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ":
        await MessageHandler.handle_feedback(message, state)
    elif message.text == "ğŸ’¬ ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹":
        await MessageHandler.handle_contacts(message)



# Define states
class BotStates(StatesGroup):
    waiting_for_feedback = State()
    waiting_for_room_number = State()
    waiting_for_find_room = State()
    asking_question = State()

# Helper functions for Pinecone integration
async def get_vector_from_text(text: str) -> List[float]:
    try:
        if not text or len(text) < 3:  # Check if the text is too short
            raise ValueError("Text is empty or too short to generate a meaningful embedding.")

        # Tokenize and chunk the text
        tokenizer = tiktoken.encoding_for_model(model_name="text-embedding-ada-002", )
        tokens = tokenizer.encode(text)

        max_tokens = 8192
        chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]

        vectors = []
        for chunk in chunks:
            chunk_text = tokenizer.decode(chunk)
            response = await openai.Embedding.acreate(
                input=chunk_text,
                model="text-embedding-ada-002"
            )
            embedding = response["data"][0]["embedding"]

            if any(map(lambda x: x != x, embedding)):  # Check for NaN
                logger.error("Generated embedding contains NaN values.")
                continue

            vectors.append(embedding)

        if not vectors:
            raise ValueError("No valid embeddings were generated.")

        averaged_vector = [sum(x)/len(x) for x in zip(*vectors)]
        return averaged_vector

    except Exception as e:
        logger.error(f"Error generating embedding: {e}")
        raise ValueError("No valid embeddings were generated.")


async def add_vectors_to_pinecone(vectors: List[Dict[str, Any]]):
    try:
        for vector in vectors:
            # Truncate the metadata if it exceeds the limit
            if len(vector['metadata']['text']) > 40960:
                vector['metadata']['text'] = vector['metadata']['text'][:40000] + '...'

        response = index.upsert(vectors)
    except Exception as e:
        logger.error(f"Error adding vectors to Pinecone: {e}")
        raise



async def query_pinecone(vector: List[float]) -> str:
    results = index.query(vector=vector, top_k=100, include_metadata=True)
    contexts = [match["metadata"]["text"] for match in results["matches"]]

    return "\n\n".join(contexts) if contexts else "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ñ Ğ½Ğµ ÑĞ¼Ğ¾Ğ³ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚."

async def generate_answer_from_context(question: str, context: str) -> str:
    try:
#         model = genai.GenerativeModel(
#             model_name="gemini-1.5-flash",
#             system_instruction=""""You are a friendly, empathetic, and knowledgeable mentor's assistant at KBTU, guiding first-year students. Your primary role is to provide clear, structured, and supportive answers based on the documents and information provided.
#
# 1. **Focus equally on academic guidance, technical support, and emotional assistance** to ensure the student feels fully supported.
# 2. If a question touches on a sensitive topic, **respond with empathy** and suggest seeking help from a professional like a psychologist or mentor if needed.
# 3. If you cannot find the relevant information in the documents, **acknowledge this** and gently recommend that the student reach out to their mentor for further assistance.
# 4. Keep the **tone casual, empathetic, and supportive**, avoiding overly formal language. Use simple language and **emojis** where appropriate to make the conversation engaging and friendly.
#
# **Handling Different Terms:**
# - Sometimes, students might use terms in Cyrillic that are meant to represent concepts commonly written in Latin script (e.g., 'Ğ°Ğ´Ğ´ Ğ´Ñ€Ğ¾Ğ¿' for 'Add/drop'). Recognize these terms and respond accordingly by matching them to their Latin equivalents when possible.
#
# Example Interaction:
# - Student: 'ĞšĞ°Ğº Ğ¼Ğ½Ğµ Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ñ€ĞµÑ‚ĞµĞ¹Ğº?'
# - Bot: 'Ğ§Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ñ€ĞµÑ‚ĞµĞ¹Ğº, Ñ‚ĞµĞ±Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾... ğŸ˜Š'
#
# If you encounter a term like 'Ğ°Ğ´Ğ´ Ğ´Ñ€Ğ¾Ğ¿', recognize it as 'Add/drop' and provide the appropriate guidance.
#
# Always strive to make the student feel supported, understood, and encouraged."
# """
#         )
#         response = model.generate_content(f"Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {question}\nĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚: {context}")
        response = await openai.ChatCompletion.acreate(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": """
You are a friendly, empathetic, and knowledgeable mentor's assistant at School of Information technology and Engineering, also known as "Site", "Ğ¨Ğ˜Ğ¢Ğ¸Ğ˜", "Ğ¤Ğ˜Ğ¢" at KBTU, guiding first-year students. Your primary role is to provide clear, structured, and supportive answers based on the documents and information provided without using asterisks in text.
Your goal - helps for mentors and in cases where you are not sure of the answer, refer the student to your mentor.
1. Focus equally on academic guidance, technical support, and emotional assistance to ensure the student feels fully supported.
2. If a question touches on a sensitive topic, respond with empathy and suggest seeking help from a professional like a psychologist or mentor if needed.
3. If you cannot find the relevant information in the documents, acknowledge this and gently recommend that the student reach out to their mentor for further assistance.
4. Keep the tone casual, empathetic, and supportive, avoiding overly formal language. Use simple language and emojis where appropriate to make the conversation engaging and friendly.
5. If a student asks for help in choosing elective courses (also referred to as "ÑĞ»ĞµĞºÑ‚Ğ¸Ğ²ĞºĞ¸" or "Ğ¼Ğ°Ğ¶Ğ¾Ñ€ĞºĞ¸"), provide them with a list of 6 elective courses on english with their codes that best match their request and describe reason, based on the information stored in the database. Always ensure these recommendations are directly sourced from the available documents.

Handling Different Terms:
- Sometimes, students might use terms in Cyrillic that are meant to represent concepts commonly written in Latin script (e.g., 'Ğ°Ğ´Ğ´ Ğ´Ñ€Ğ¾Ğ¿' for 'Add/drop'). Recognize these terms and respond accordingly by matching them to their Latin equivalents when possible.

Example Interaction:
- Student: 'ĞšĞ°Ğº Ğ¼Ğ½Ğµ Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ñ€ĞµÑ‚ĞµĞ¹Ğº?'
- Bot: 'Ğ§Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ñ€ĞµÑ‚ĞµĞ¹Ğº, Ñ‚ĞµĞ±Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾... ğŸ˜Š'

If you encounter a term like 'Ğ°Ğ´Ğ´ Ğ´Ñ€Ğ¾Ğ¿', recognize it as 'Add/drop' and provide the appropriate guidance. Or if you encounter a term like "Ğ¾Ñ€", recognize it as "ĞÑ„Ğ¸Ñ Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ°" and provide the appropriate guidance.

Always strive to make the student feel supported, understood, and encouraged.
"""},
                {"role": "user", "content": f"Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {question}\nĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚: {context}"},
            ],
        )
        # response_text = response.text.strip()
        response_text = response.choices[0].message["content"].strip()
        response_text = re.sub(r'#\s*', '', response_text)
        # logger.debug(f"\n\n========CONTEXT===========\n\n{context}")
        return response_text
    except Exception as e:
        logger.error(f"Error in OpenAI request: {e}")
        return "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°."


class FileHandler:

    UPLOAD_DIR = "./uploads/"

    @staticmethod
    def generate_ascii_id(file_name: str) -> str:
        hash_object = hashlib.sha256(file_name.encode('utf-8'))
        return hash_object.hexdigest()

    @staticmethod
    async def handle_file_upload(message: types.Message):
        if not await UserManager.is_admin(message.from_user.id):
            await bot.send_message(message.from_user.id, "Ğ£ Ğ²Ğ°Ñ Ğ½ĞµÑ‚ Ğ¿Ñ€Ğ°Ğ² Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ².")
            return

        documents = message.document if isinstance(message.document, list) else [message.document]
        uploaded_files = []
        for document in documents:
            file = await bot.get_file(document.file_id)
            file_path = os.path.join(FileHandler.UPLOAD_DIR, document.file_name)
            await bot.download_file(file.file_path, file_path)

            # Process and store the document
            await FileHandler.process_and_store_document(file_path, document.file_name, message.from_user.id)
            uploaded_files.append(document.file_name)

        await bot.send_message(message.from_user.id, f"Ğ’ÑĞµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ñ‹:\n" + "\n".join(uploaded_files))

    @staticmethod
    async def process_and_store_document(file_path: str, file_name: str, user_id: int):
        text = None
        if file_path.endswith('.docx'):
            text = FileHandler.extract_text_from_docx(file_path)
        elif file_path.endswith('.pdf'):
            text = FileHandler.extract_text_from_pdf(file_path)
        elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):
            text = FileHandler.extract_text_from_excel(file_path)
        else:
            try:
                async with aiofiles.open(file_path, mode='r', encoding='utf-8') as file:
                    text = await file.read()
            except UnicodeDecodeError:
                try:
                    async with aiofiles.open(file_path, mode='r', encoding='cp1251') as file:
                        text = await file.read()
                except UnicodeDecodeError:
                    async with aiofiles.open(file_path, mode='rb') as file:
                        text = await file.read()

        if isinstance(text, bytes):
            text = text.decode('utf-8', errors='ignore')

        text = text.strip()

        if not text:
            logger.error("Text is empty or invalid after reading from file.")
            await bot.send_message(user_id, "ĞÑˆĞ¸Ğ±ĞºĞ°: Ñ‚ĞµĞºÑÑ‚ Ğ² Ñ„Ğ°Ğ¹Ğ»Ğµ Ğ¿ÑƒÑÑ‚ Ğ¸Ğ»Ğ¸ Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚ĞµĞ½.")
            return

        # Split the text into smaller chunks
        chunk_size = 4000  # Adjust this size based on your needs
        text_chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

        try:
            for i, chunk in enumerate(text_chunks):
                vector = await get_vector_from_text(chunk)
                chunk_id = f"{FileHandler.generate_ascii_id(file_name)}_{i}"
                vectors = [
                    {"id": chunk_id, "values": vector, "metadata": {"text": chunk[:2000], "filename": file_name}}]


                await add_vectors_to_pinecone(vectors)

        except ValueError as ve:
            logger.error(f"Failed to process document: {ve}")
            await bot.send_message(user_id, "ĞÑˆĞ¸Ğ±ĞºĞ°: Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ».")
        except Exception as e:
            logger.error(f"Unexpected error processing document: {e}")
            await bot.send_message(user_id, "ĞÑˆĞ¸Ğ±ĞºĞ°: Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ„Ğ°Ğ¹Ğ».")

    @staticmethod
    def extract_text_from_docx(file_path: str) -> str:
        doc = docx.Document(file_path)
        full_text = []
        for para in doc.paragraphs:
            full_text.append(para.text)
        return '\n'.join(full_text)

    @staticmethod
    def  extract_text_from_pdf(file_path: str) -> str:
        full_text = []
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            for page in reader.pages:
                full_text.append(page.extract_text())
        return '\n'.join(full_text)

    @staticmethod
    def extract_text_from_excel(file_path: str) -> str:
        wb = openpyxl.load_workbook(file_path)
        full_text = []
        for sheet in wb:
            for row in sheet.iter_rows(values_only=True):
                row_text = " ".join([str(cell) for cell in row if cell is not None])
                full_text.append(row_text)
        return '\n'.join(full_text)

    @staticmethod
    async def delete_vectors_by_filename(filename: str):
        try:
            vector_id = FileHandler.generate_ascii_id(filename)
            index.delete(ids=[vector_id])
            logger.debug(f"Deleted vector with ID: {vector_id}")
        except Exception as e:
            logger.error(f"Error deleting vector: {e}")
            raise

    @staticmethod
    async def delete_file(filename: str):
        file_path = os.path.join(FileHandler.UPLOAD_DIR, filename)
        if os.path.exists(file_path):
            os.remove(file_path)
            logger.debug(f"Deleted file: {file_path}")
        else:
            logger.error(f"File not found: {file_path}")

    @staticmethod
    async def handle_delete_request(message: types.Message, filename: str):
        await FileHandler.delete_vectors_by_filename(filename)
        await FileHandler.delete_file(filename)
        await bot.send_message(message.from_user.id, f"Ğ¤Ğ°Ğ¹Ğ» '{filename}' Ğ¸ ĞµĞ³Ğ¾ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ñ‹.")


class MessageHandler:
    @staticmethod
    async def send_welcome(message: types.Message):
        await UserManager.add_user(
            user_id=message.from_user.id,
            username=message.from_user.username,
            first_name=message.from_user.first_name
        )
        first_name = message.from_user.first_name or "ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ"
        welcome_msg = (
            f"ĞŸÑ€Ğ¸Ğ²ĞµÑ‚, {first_name}! \n\n"
            f"ğŸ¤– Ğ­Ñ‚Ğ¾ Ğ±Ğ¾Ñ‚ Ğ´Ğ»Ñ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ğ¸ ĞŸĞµÑ€Ğ²Ğ°ÑˆĞ° Ğ² ÑÑ‚ĞµĞ½Ğ°Ñ… ĞšĞ‘Ğ¢Ğ£. "
            f"Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ·Ğ°Ğ½Ğ¸Ğ¼Ğ°ÑÑ‰ĞµĞ¹ÑÑ Ğ¼ĞµĞ½Ñ‚Ğ¾Ñ€ÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ¾Ğ¹.\n\n"
            f"ğŸ” Ğ—Ğ´ĞµÑÑŒ Ğ²Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ:\n"
            f"- Ğ£Ğ·Ğ½Ğ°Ñ‚ÑŒ Ğ³Ğ´Ğµ Ğ²Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¶Ğµ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¹ ĞºĞ°Ğ±Ğ¸Ğ½ĞµÑ‚\n"
            f"- Ğ Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¹ ÑƒÑ‡ĞµĞ±Ğ½Ñ‹Ğ¹ Ğ¿Ğ»Ğ°Ğ½ Ğ¨Ğ˜Ğ¢Ğ¸Ğ˜\n"
            f"- Ğ—Ğ°Ğ´Ğ°Ñ‚ÑŒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ñ‹ ÑƒĞ¶Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ğ»Ğ¸ Ğ² ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ.\n"
            f"- ĞÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¶Ğ°Ğ»Ğ¾Ğ±Ñƒ/Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ğ”ĞµĞºĞ°Ğ½Ğ°Ñ‚Ñƒ Ğ¸Ğ»Ğ¸ Ğ¼ĞµĞ½Ñ‚Ğ¾Ñ€ÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğµ\n"
        )
        await bot.send_message(message.from_user.id, welcome_msg, reply_markup=main_keyboard)

    @staticmethod
    async def handle_map(message: types.Message):
        maps_paths = [
            "./map/1-floor_kbtu.jpg",
            "./map/2-floor_kbtu.jpg",
            "./map/3-floor_kbtu.jpg",
            "./map/4-floor_kbtu.jpg",
            "./map/5-floor_kbtu.jpg",
        ]
        media = [InputMediaPhoto(media=FSInputFile(map_path)) for map_path in maps_paths]

        await bot.send_media_group(message.from_user.id, media)

    @staticmethod
    async def handle_contacts(message: types.Message):
        response_text = "Ğ¡Ğ¾Ñ† ÑĞµÑ‚Ğ¸: \n\n â„–1 Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¸Ì† Ğ²ÑƒĞ· ĞšĞ°Ğ·Ğ°Ñ…ÑÑ‚Ğ°Ğ½Ğ° (@kbtu_official) â€¢ Ğ¤Ğ¾Ñ‚Ğ¾ Ğ¸ Ğ²Ğ¸Ğ´ĞµĞ¾ Ğ² InstagramTelegram: Contact @kbtuadmission2024(23) ĞŸÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ | LinkedInKBTU (@kbtu_official) | TikTok \n\n Ğ›Ğ¾Ğ³Ğ¸Ğ½ Ğ¸ Ğ¿Ğ°Ñ€Ğ¾Ğ»ÑŒ â€“ Helpingstudents@kbtu.kz\n\n ğŸ“ ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹:\n- ĞÑ„Ğ¸Ñ Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ°: 8 727 357 42 81, d.fazylova@kbtu.kz, officeregistrar@kbtu.kz\n- Ğ‘Ğ¸Ğ±Ğ»Ğ¸Ğ¾Ñ‚ĞµĞºĞ°: 8 727 357 42 84 (Ğ²Ğ½. 241), u.bafubaeva@kbtu.kz\n- ĞĞ±Ñ‰ĞµĞ¶Ğ¸Ñ‚Ğ¸Ğµ: 8 727 357 42 42 (Ğ²Ğ½. 601), m.shopanov@kbtu.kz, a.esimbekova@kbtu.kz\n- ĞĞ¿Ğ»Ğ°Ñ‚Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: 8 727 357 42 58 (Ğ²Ğ½. 163, 169) a.nauruzbaeva@kbtu.kz, m.aitakyn@kbtu.kz\n- ĞœĞµĞ´. Ñ†ĞµĞ½Ñ‚Ñ€ - medcenter@kbtu.kz\n\nğŸ« Ğ”ĞµĞºĞ°Ğ½Ğ°Ñ‚Ñ‹:\n- Ğ‘Ğ¸Ğ·Ğ½ĞµÑ ÑˆĞºĞ¾Ğ»Ğ°: 8 727 357 42 67 (Ğ²Ğ½. 352, 358), e.mukashev@kbtu.kz, a.yerdebayeva@kbtu.kz\n- ĞœĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ°Ñ ÑˆĞºĞ¾Ğ»Ğ° ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸ĞºĞ¸: 8 727 357 42 71 (Ğ²Ğ½. 383), a.islyami@kbtu.kz, d.bisenbaeva@kbtu.kz\n- Ğ¨ĞºĞ¾Ğ»Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ Ğ¸ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸: 8 727 357 42 20, fit_1course@kbtu.kz\n- Ğ¨ĞºĞ¾Ğ»Ğ° Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´Ğ½Ğ¾Ğ¹ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ¸: 8 727 357 42 25, a.isakhov@kbtu.kz, n.eren@kbtu.kz\n- Ğ¨ĞºĞ¾Ğ»Ğ° ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ½ĞµÑ„Ñ‚ĞµĞ³Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ¹ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ¸: 8 727 357 42 42 (Ğ²Ğ½. 324), a.ismailov@kbtu.kz, a.abdukarimov@kbtu.kz\n- Ğ¨ĞºĞ¾Ğ»Ğ° Ğ³ĞµĞ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸: 8 727 357 42 42 (Ğ²Ğ½. 326), a.akhmetzhanov@kbtu.kz, g.ulkhanova@kbtu.kz\n- ĞšĞ°Ğ·Ğ°Ñ…ÑÑ‚Ğ°Ğ½ÑĞºĞ°Ñ Ğ¼Ğ¾Ñ€ÑĞºĞ°Ñ Ğ°ĞºĞ°Ğ´ĞµĞ¼Ğ¸Ñ: 8 727 357 42 27 (Ğ²Ğ½. 390, 392), r.biktashev@kbtu.kz, s.dlimbetova@kbtu.kz\n- Ğ¨ĞºĞ¾Ğ»Ğ° Ñ…Ğ¸Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ğ¸Ğ½Ğ¶ĞµĞ½ĞµÑ€Ğ¸Ğ¸: 8 727 291 57 84, +8 727 357 42 42 (Ğ²Ğ½. 492), k.dzhamansarieva@kbtu.kz, n.saparbaeva@kbtu.kz\n- Ğ›Ğ°Ğ±Ğ¾Ñ€Ğ°Ñ‚Ğ¾Ñ€Ğ¸Ñ Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ½Ğ°Ğ½Ğ¾Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹: 8 727 357 42 66 (Ğ²Ğ½. 550), n.beisenkhanov@kbtu.kz, z.bugybai@kbtu.kz\n"
        await bot.send_message(message.chat.id, response_text)

    @staticmethod
    async def handle_feedback(message: types.Message, state: FSMContext):
        await bot.send_message(
            message.from_user.id,
            "Ğ—Ğ´ĞµÑÑŒ Ğ²Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ°Ğ½Ğ¾Ğ½Ğ¸Ğ¼Ğ½Ğ¾ Ğ½Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¶Ğ°Ğ»Ğ¾Ğ±Ñƒ, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸Ğ»Ğ¸ Ğ¶Ğµ ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑĞ¿Ğ°ÑĞ¸Ğ±Ğ¾ Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞœĞµĞ½Ñ‚Ğ¾Ñ€ÑĞºĞ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğµ, Ğ° Ñ‚Ğ°Ğº Ğ¶Ğµ Ğ”ĞµĞºĞ°Ğ½Ğ°Ñ‚Ñƒ Ğ¨Ğ˜Ğ¢Ğ¸Ğ˜. Ğ’ÑĞµ Ğ¿Ğ¸ÑÑŒĞ¼Ğ° Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒÑÑ Ğ¸ Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒÑÑ. Ğ£Ğ´Ğ°Ñ‡Ğ¸!",
        )
        await state.set_state(BotStates.waiting_for_feedback)

    @staticmethod
    async def process_feedback(message: types.Message, state: FSMContext):
        await UserManager.save_feedback(message.from_user.id, message.text)
        await bot.send_message(message.from_user.id, "Ğ¡Ğ¿Ğ°ÑĞ¸Ğ±Ğ¾ Ğ·Ğ° Ğ²Ğ°ÑˆĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ!")
        await state.clear()

    @staticmethod
    async def handle_rup(message: types.Message):
        rup_keyboard = ReplyKeyboardMarkup(
            keyboard=[
                [KeyboardButton(text="Ğ’Ğ¢Ğ˜ĞŸĞ"), KeyboardButton(text="Ğ˜Ğ¡")],
                [KeyboardButton(text="ĞĞ˜Ğ£"), KeyboardButton(text="Ğ Ğ˜Ğœ"), KeyboardButton(text="IT management")],
                [KeyboardButton(text="ĞĞ°Ğ·Ğ°Ğ´")]
            ],
            resize_keyboard=True
        )

        await bot.send_message(
            message.from_user.id, "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¹ Ğ²Ğ°Ğ¼ Ğ Ğ£ĞŸ:", reply_markup=rup_keyboard
        )

    @staticmethod
    async def handle_rup_options(message: types.Message):
        file_paths = {
            "Ğ’Ğ¢Ğ˜ĞŸĞ": "./rup_fit/VTIPO.pdf",
            "Ğ˜Ğ¡": "./rup_fit/IS.pdf",
            "Ğ Ğ˜Ğœ": "./rup_fit/RIM.pdf",
            "ĞĞ˜Ğ£": "./rup_fit/AU.pdf",
            "IT management": "./rup_fit/it_man.pdf"
        }
        file_path = file_paths.get(message.text)
        if file_path:
            await bot.send_document(message.from_user.id, FSInputFile(file_path))



    @staticmethod
    async def handle_back(message: types.Message):
        await UserManager.send_main_keyboard(message.from_user.id)

    @staticmethod
    async def ask_for_room_number(message: types.Message, state: FSMContext):
        await bot.send_message(message.from_user.id, "Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ½Ğ¾Ğ¼ĞµÑ€ ĞºĞ°Ğ±Ğ¸Ğ½ĞµÑ‚Ğ° Ñ€ÑĞ´Ğ¾Ğ¼ Ñ Ğ²Ğ°Ğ¼Ğ¸.")
        await state.set_state(BotStates.waiting_for_room_number)

    @staticmethod
    async def handle_room_number(message: types.Message, state: FSMContext):
        found = False
        room_mapping = {
            range(100, 144): ("Ğ’Ñ‹ Ğ½Ğ° ĞŸĞ°Ğ½Ñ„Ğ¸Ğ»Ğ¾Ğ²Ğ°, 1 ÑÑ‚Ğ°Ğ¶.", "./map/floor1/PF.png"),
            range(144, 153): ("Ğ’Ñ‹ Ğ½Ğ° Ğ¢Ğ¾Ğ»Ğµ Ğ‘Ğ¸, 1 ÑÑ‚Ğ°Ğ¶.", "./map/floor1/TB.png"),
            range(156, 184): ("Ğ’Ñ‹ Ğ½Ğ° ĞĞ±Ñ‹Ğ»Ğ°Ğ¹Ñ…Ğ°Ğ½Ğ°, 1 ÑÑ‚Ğ°Ğ¶.", "./map/floor1/Abl.png"),
            range(252, 285): ("Ğ’Ñ‹ Ğ½Ğ° ĞĞ±Ñ‹Ğ»Ğ°Ğ¹Ñ…Ğ°Ğ½Ğ°, 2 ÑÑ‚Ğ°Ğ¶.", "./map/floor2/ABL.png"),
            range(202, 246): ("Ğ’Ñ‹ Ğ½Ğ° ĞŸĞ°Ğ½Ñ„Ğ¸Ğ»Ğ¾Ğ²Ğ°, 2 ÑÑ‚Ğ°Ğ¶.", "./map/floor2/PF.png"),
            range(246, 252): ("Ğ’Ñ‹ Ğ½Ğ° Ğ¢Ğ¾Ğ»Ğµ Ğ‘Ğ¸, 2 ÑÑ‚Ğ°Ğ¶.", "./map/floor2/TB.png"),
            range(300, 344): ("Ğ’Ñ‹ Ğ½Ğ° ĞŸĞ°Ğ½Ñ„Ğ¸Ğ»Ğ¾Ğ²Ğ°, 3 ÑÑ‚Ğ°Ğ¶.", "./map/floor3/PF.png"),
            range(344, 361): ("Ğ’Ñ‹ Ğ½Ğ° Ğ¢Ğ¾Ğ»Ğµ Ğ‘Ğ¸, 3 ÑÑ‚Ğ°Ğ¶.", "./map/floor3/TB.png"),
            range(361, 389): ("Ğ’Ñ‹ Ğ½Ğ° ĞĞ±Ñ‹Ğ»Ğ°Ğ¹Ñ…Ğ°Ğ½Ğ°, 3 ÑÑ‚Ğ°Ğ¶.", "./map/floor3/ABL.png"),
            range(501, 523): ("Ğ’Ñ‹ Ğ½Ğ° Ğ¢Ğ¾Ğ»Ğµ Ğ‘Ğ¸, 5 ÑÑ‚Ğ°Ğ¶.", "./map/5floor.png"),
            range(400, 417): ("Ğ’Ñ‹ Ğ½Ğ° ĞŸĞ°Ğ½Ñ„Ğ¸Ğ»Ğ¾Ğ²Ğ°, 4 ÑÑ‚Ğ°Ğ¶.", "./map/floor4/PF.png"),
            range(419, 439): ("Ğ’Ñ‹ Ğ½Ğ° ĞšĞ°Ğ·Ñ‹Ğ±ĞµĞº Ğ‘Ğ¸, 4 ÑÑ‚Ğ°Ğ¶.", "./map/floor4/KB.png"),
            range(444, 462): ("Ğ’Ñ‹ Ğ½Ğ° ĞĞ±Ñ‹Ğ»Ğ°Ğ¹Ñ…Ğ°Ğ½Ğ°, 4 ÑÑ‚Ğ°Ğ¶.", "./map/floor4/ABL.png"),
            range(462, 477): ("Ğ’Ñ‹ Ğ½Ğ° Ğ¢Ğ¾Ğ»Ğµ Ğ‘Ğ¸, 4 ÑÑ‚Ğ°Ğ¶.", "./map/floor4/TB.png"),
        }

        if message.text.isdigit():
            room_number = int(message.text)
            for room_range, (location, map_path) in room_mapping.items():
                if room_number in room_range:
                    await bot.send_message(message.from_user.id, location)
                    await bot.send_photo(message.from_user.id, FSInputFile(map_path))
                    found = True
                    break

        if not found:
            await bot.send_message(
                message.from_user.id,
                "ĞĞ¾Ğ¼ĞµÑ€ ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚Ñ‹ Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¾. ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ.",
            )

        await state.clear()

    @staticmethod
    async def handle_find_room(message: types.Message, state: FSMContext):
        await bot.send_message(
            message.from_user.id,
            "Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ğ±Ğ¸Ğ½ĞµÑ‚Ğ°, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ²Ñ‹ Ñ…Ğ¾Ñ‚Ğ¸Ñ‚Ğµ Ğ½Ğ°Ğ¹Ñ‚Ğ¸.\n\nĞœĞ¾Ğ¶Ğ½Ğ¾ ÑƒĞ·Ğ½Ğ°Ñ‚ÑŒ Ğ¼ĞµÑÑ‚Ğ° Ğ³Ğ´Ğµ Ñ€ÑĞ´Ğ¾Ğ¼ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾ĞºÑƒÑˆĞ°Ñ‚ÑŒ Ğ¸Ğ»Ğ¸ Ğ¶Ğµ Ğ¿Ğ¾ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ",
        )
        await state.set_state(BotStates.waiting_for_find_room)

    @staticmethod
    async def process_find_room(message: types.Message, state: FSMContext):
        found = False
        location_mapping = {
            "Ñ…Ğ°Ğ»Ñ‹Ğº": ("Ğ­Ñ‚Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞšĞ°Ğ·Ñ‹Ğ±ĞµĞº Ğ±Ğ¸, 1 ÑÑ‚Ğ°Ğ¶.", "./map/floor1/KB.png"),
            "Ğ³ĞµĞ¹Ğ¼Ğ´ĞµĞ²": ("Ğ­Ñ‚Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞšĞ°Ğ·Ñ‹Ğ±ĞµĞº Ğ±Ğ¸, 2 ÑÑ‚Ğ°Ğ¶Ğµ.", "./map/floor2/KB.png"),
            "ĞºĞ¾Ğ²Ğ¾Ñ€ĞºĞ¸Ğ½Ğ³ Ğ½Ğ° 2 ÑÑ‚Ğ°Ğ¶Ğµ": ("Ğ­Ñ‚Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞšĞ°Ğ·Ñ‹Ğ±ĞµĞº Ğ±Ğ¸, 2 ÑÑ‚Ğ°Ğ¶Ğµ.", "./map/floor2/KB.png"),
            "game dev": ("Ğ­Ñ‚Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞšĞ°Ğ·Ñ‹Ğ±ĞµĞº Ğ±Ğ¸, 2 ÑÑ‚Ğ°Ğ¶Ğµ.", "./map/floor2/KB.png"),
            "gamedev": ("Ğ­Ñ‚Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞšĞ°Ğ·Ñ‹Ğ±ĞµĞº Ğ±Ğ¸, 2 ÑÑ‚Ğ°Ğ¶Ğµ.", "./map/floor2/KB.png"),
            "726": ("Ğ­Ñ‚Ğ¾ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒÑÑ Ğ½Ğ° ĞšĞ°Ğ·Ñ‹Ğ±ĞµĞº Ğ±Ğ¸, 2 ÑÑ‚Ğ°Ğ¶Ğµ.", "./map/floor2/KB.png"),
            "ÑÑ‚Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°": ("Ğ¡Ñ‚Ğ¾Ğ»Ğ¾Ğ²ĞºĞ° Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ½Ğ° 0 ÑÑ‚Ğ°Ğ¶Ğµ Ğ¢Ğ¾Ğ»Ğµ Ğ‘Ğ¸.", "./map/floor1/Canteen.png"),
        }

        room_number_mapping = {
            "ĞºÑƒÑˆĞ°Ñ‚ÑŒ": "Ğ¡Ñ‚Ğ¾Ğ»Ğ¾Ğ²ĞºĞ° Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ½Ğ° 0 ÑÑ‚Ğ°Ğ¶Ğµ Ğ¢Ğ¾Ğ»Ğµ Ğ‘Ğ¸. ĞšÑƒĞ¿Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞºÑƒÑ Ğ½Ğ° 0, 1, 3 ÑÑ‚Ğ°Ğ¶Ğµ Ğ¢Ğ¾Ğ»Ğµ Ğ‘Ğ¸, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ½Ğ° 1 ÑÑ‚Ğ°Ğ¶Ğµ ĞĞ±Ñ‹Ğ»Ğ°Ğ¹Ñ…Ğ°Ğ½Ğ°. Ğ•Ñ‰Ğµ Ñ€ÑĞ´Ğ¾Ğ¼ Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€Ğ¾Ğ¼ ĞµÑÑ‚ÑŒ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğ¹, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾ĞºÑƒÑˆĞ°Ñ‚ÑŒ.",
            "ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ": "ĞŸĞ¾ĞºĞ° Ñ Ğ½Ğµ Ğ·Ğ½Ğ°Ñ, Ğ³Ğ´Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚ÑŒÑÑ",
        }

        if message.text.lower() in room_number_mapping:
            response = room_number_mapping[message.text.lower()]
            await bot.send_message(message.from_user.id, response, reply_markup=main_keyboard)
            found = True
        elif message.text.isdigit():
            await MessageHandler.handle_room_number(message, state)
            found = True
        elif message.text.lower() in location_mapping:
            location, map_path = location_mapping[message.text.lower()]
            await bot.send_message(message.from_user.id, location)
            await bot.send_photo(message.from_user.id, FSInputFile(map_path))
            found = True

        if not found:
            await bot.send_message(
                message.from_user.id,
                "ĞĞ¾Ğ¼ĞµÑ€ ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚Ñ‹ Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¾. ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ñ‹Ğ¹ Ğ½Ğ¾Ğ¼ĞµÑ€ Ğ¸Ğ»Ğ¸ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ.",
            )


    @staticmethod
    async def handle_langchain_question(message: types.Message, state: FSMContext):
        await bot.send_message(
            message.from_user.id, "Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ²Ğ°Ñˆ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ:", reply_markup=chatgpt_keyboard
        )
        if state:
            await state.set_state(BotStates.asking_question)

    @staticmethod
    async def process_langchain_question(message: types.Message, state: FSMContext):
        if message.text in {"ğŸ—ºï¸ ĞšĞ°Ñ€Ñ‚Ğ°", "ğŸ“š Ğ Ğ£ĞŸ Ğ¨Ğ˜Ğ¢Ğ¸Ğ˜", "ğŸ«£ Ğ“Ğ´Ğµ Ğ¯?", "ğŸ” ĞĞ°Ğ¹Ñ‚Ğ¸", "ğŸ“¥ Ğ–Ğ°Ğ»Ğ¾Ğ±Ñ‹/ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ", "ğŸ’¬ ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹"}:
            await handle_main_menu_button(message, state)
            return

        if message.text == "â¬…ï¸ ĞĞ°Ğ·Ğ°Ğ´":
            await state.clear()
            await UserManager.send_main_keyboard(message.from_user.id)
            return

        await bot.send_message(
            message.from_user.id,
            "Ğ’Ğ°Ñˆ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚. ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¿Ğ¾Ğ´Ğ¾Ğ¶Ğ´Ğ¸Ñ‚Ğµ, ÑÑ‚Ğ¾ Ğ·Ğ°Ğ¹Ğ¼ĞµÑ‚ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ.",
        )

        # Get the question
        question = message.text



        try:
            query_vector = await get_vector_from_text(question)
        except ValueError as ve:
            logger.error(f"Error generating embedding: {ve}")
            await bot.send_message(message.from_user.id, "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ñ Ğ½Ğµ ÑĞ¼Ğ¾Ğ³ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚.")
            return


        try:
            context = await query_pinecone(query_vector)
        except Exception as e:
            logger.error(f"Error querying Pinecone: {e}")
            await bot.send_message(message.from_user.id, "Ğ˜Ğ·Ğ²Ğ¸Ğ½Ğ¸Ñ‚Ğµ, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°.")
            return

        # Generate an answer based on the context
        response = await generate_answer_from_context(question, context)
        response = response.replace('*', '')

        await bot.send_message(message.from_user.id, response)

    @staticmethod
    async def handle_unhandled_message(message: types.Message, state: FSMContext):
        if message.document:
            await FileHandler.handle_file_upload(message)
        else:
            if state is None or state.get_state() is None:
                # If no state is set, assume the user is asking a question
                await state.set_state(BotStates.asking_question)
                await MessageHandler.handle_langchain_question(message, state)
            else:
                await MessageHandler.process_langchain_question(message, state)




    @staticmethod
    async def handle_back(message: types.Message, state: FSMContext):
        await bot.send_message(
            message.from_user.id,
            "Ğ’Ñ‹ Ğ²ĞµÑ€Ğ½ÑƒĞ»Ğ¸ÑÑŒ Ğ½Ğ°Ğ·Ğ°Ğ´.",
            reply_markup=main_keyboard,
        )
        await state.clear()

class UserManager:
    @staticmethod
    async def add_user(user_id: int, username: str = None, first_name: str = None):
        user_name = username or first_name or f"user_{user_id}"
        await users_collection.update_one(
            {"user_id": user_id},
            {"$set": {"user_id": user_id, "user_name": user_name}},
            upsert=True
        )

    @staticmethod
    async def is_admin(user_id: int) -> bool:
        return user_id in ADMINS

    @staticmethod
    async def save_feedback(user_id: int, feedback: str):
        await feedback_collection.insert_one({
            "user_id": user_id,
            "feedback": feedback
        })

    @staticmethod
    async def send_main_keyboard(user_id: int):
        await bot.send_message(user_id, "Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ¾Ğ¿Ñ†Ğ¸Ñ:", reply_markup=main_keyboard)

# Register handlers
@router.message(Command("start"))
async def start_command(message: types.Message):
    await MessageHandler.send_welcome(message)

@router.message(F.text == "ĞšĞ°Ñ€Ñ‚Ğ°ğŸ—ºï¸")
async def map_command(message: types.Message):
    await MessageHandler.handle_map(message)


@router.message(F.text == "ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹ğŸ’¬")
async def contacts_command(message: types.Message):
    await MessageHandler.handle_contacts(message)


@router.message(F.text == "Ğ–Ğ°Ğ»Ğ¾Ğ±Ñ‹/ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ÑğŸ“¥")
async def feedback_command(message: types.Message, state: FSMContext):
    await MessageHandler.handle_feedback(message, state)

@router.message(BotStates.waiting_for_feedback)
async def process_feedback(message: types.Message, state: FSMContext):
    await MessageHandler.process_feedback(message, state)


@router.message(F.text == "Ğ Ğ£ĞŸ Ğ¨Ğ˜Ğ¢Ğ¸Ğ˜ğŸ“š")
async def rup_command(message: types.Message):
    await MessageHandler.handle_rup(message)


@router.message(F.text.in_({"Ğ’Ğ¢Ğ˜ĞŸĞ", "Ğ˜Ğ¡", "ĞĞ˜Ğ£", "Ğ Ğ˜Ğœ", "IT management"}))
async def rup_option_command(message: types.Message):
    await MessageHandler.handle_rup_options(message)


@router.message(F.text == "ĞĞ°Ğ·Ğ°Ğ´")
async def back_command(message: types.Message, state: FSMContext):
    await MessageHandler.handle_back(message, state)

@router.message(F.text.in_({"ğŸ—ºï¸ ĞšĞ°Ñ€Ñ‚Ğ°", "ğŸ“š Ğ Ğ£ĞŸ Ğ¨Ğ˜Ğ¢Ğ¸Ğ˜", "ğŸ«£ Ğ“Ğ´Ğµ Ğ¯?", "ğŸ” ĞĞ°Ğ¹Ñ‚Ğ¸", "ğŸ“¥ Ğ–Ğ°Ğ»Ğ¾Ğ±Ñ‹/ĞŸÑ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ", "ğŸ’¬ ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹"}))
async def main_menu_button_command(message: types.Message, state: FSMContext):
    await handle_main_menu_button(message, state)



@router.message(F.text == "Ğ“Ğ´Ğµ Ğ¯?ğŸ«£")
async def where_am_i_command(message: types.Message, state: FSMContext):
    await MessageHandler.ask_for_room_number(message, state)


@router.message(BotStates.waiting_for_room_number)
async def process_room_number_command(message: types.Message, state: FSMContext):
    await MessageHandler.handle_room_number(message, state)


@router.message(F.text == "ĞĞ°Ğ¹Ñ‚Ğ¸ğŸ”")
async def find_command(message: types.Message, state: FSMContext):
    await MessageHandler.handle_find_room(message, state)


@router.message(BotStates.waiting_for_find_room)
async def process_find_room_command(message: types.Message, state: FSMContext):
    await MessageHandler.process_find_room(message, state)

@router.message(F.text == "ğŸ¤– MentorGPT")
async def langchain_question_command(message: types.Message, state: FSMContext):
    await MessageHandler.handle_langchain_question(message, state)

@router.message(BotStates.asking_question)
async def process_langchain_question_command(message: types.Message, state: FSMContext):
    await MessageHandler.process_langchain_question(message, state)

@router.message()
async def handle_unhandled_message(message: types.Message, state: FSMContext):
    await MessageHandler.handle_unhandled_message(message, state)

async def main():
    dp.include_router(router)
    # await clear_pinecone_index()
    # await reprocess_uploads()
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())
